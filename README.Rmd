---
title: 'STAT/CSSS 564: Assignment 3'
author: Jeff Arnold, Sheridan Grant
output: html_document
date: "May 2nd, 2017"
---

1. Fork this repository to your account
2. Edit the file `solutions.Rmd` with your solutions to the problems.
3. Submit a pull request to have it graded. Include either or both a HTML and PDF file.

For updates and questions follow the Slack channel: [#assignment3](https://uwcsss564.slack.com/messages/C57PFSG1Z/)

This assignment will require the following R packages:
```{r, message=FALSE}
library("rstan")
library("reshape2")
```

## Problem 1

This problem is based on Bayesian Data Analysis 3 (Gelman et al.) chapter 17 exercises 2 and 3; some of the questions will be verbatim, others tweaked to more closely align with the material from this class.

As part of their analysis of the Federalist Papers, Mosteller and Wallace (1964) recorded the frequency of use of various words in selected articles by Alexander Hamilton and James Madison. The articles were divided into blocks of about 200 words each, and the number of instances of various words in each block were recorded. **may.csv** contains data on the occurence of the word "may" in these papers, with the columns indicating the number of occurences of "may" in a text block, rows the author, and entries the number of blocks in which the word "may" occurred the given number of times for the given author.

```{r}
may <- read.csv('may.csv', header = T, row.names = 1)
```

**Q1:** in Stan, fit a Bayesian Poisson GLM to these data, with a different parameter for each author and an uninformative prior. Summarize the posterior distributions for each parameter and plot histograms.

**Q2:** in Stan, fit a Negative Binomial GLM to the data with different parameters for each author and a noninformative prior distribution. Defend your choice of prior. For each author, plot the joint posterior distribution of the two associated parameters with a contour plot (elipses indicating the density around that elipse) or a density plot (colors, with darking meaning higher density at that point).

**Q3:** using the "generated quantities" block of the Stan model, generate posterior predictive distributions for both authors for both model types. Compare the posterior predictive distributions to the distribution of the actual data. Does the fit seem reasonable? Which model appears to be more reasonable?

Another reasonable way of checking model fit is to explicitly define a test statistic and perform a hypothesis test. The test statistic is, at its simplest, a function of just the data. For instance, it might be the average of the observations. To perform a hypothesis test, you simply compare the value of the test statistic from the original data to the distribution of the test statistic under the model (i.e. the posterior predictive distribution). To do this, you can simply simulate the posterior predictive many times (say, 5000) and calculate the test statistic for each simulation. If the test statistic for the observed data is far in one of the tails of the sampling from the posterior predictive, there is an indication that your model does not fit the data as well as you might hope.

**Q4:** define a test statistic for these data and justify its use (for instance, taking the sum of the cosines of the data as a test statistic would be stupid). Then, for Hamilton, perform a hypothesis test on this test statistic as outlined above, and conclude with a proper interpretation.

# Statistical Simulation

Read @KingTomzWittenberg2000a. They propose a statistical simulation approach for interpreting statistical analysis; see section "Simulation-Based Approaches to Interpretation".
Compare and contrast this to a full Bayesian approach.


# Student-t Prior

The robust regression with Student-t error example uses the following prior on the degrees of freedom parameter.
$$
\nu \sim Gamma(2, 0.01)
$$
The Student-t distribution is used because it has wider tails and thus is less sensitive to outliers than a normal distribution.
However, the researcher generally has no information about the value of the degrees of freedom.

1. Plot this prior distribution, and the values of the 5th and 95th quantiles. You can use `dgamma(x, 2, scale = 0.01)` and `qgamma(x, 2, scale = 0.01)`. What is 
2. Additionally, the prior is truncated at 2. Why? Hint: What moments of the Student-t distribution are not-defined for values between 2.


# Student-t as a Mixture of Normals

The Student-t distribution is a scale mixture of normals.^[mix]
This means that a Student-t distribution can be represented as normal distributions in which the variances are drawn from different distributions.
Suppose $X$ is distributed Student-t with degrees of freedom $\nu$, location $\mu$, and scale $\sigma$,
$$
X \sim t_{\nu}(\mu, \sigma) .
$$
Samples from $Y$ can be drawn by
$$
x_i \sim N(\mu, \lambda_i^2 \gamma^2) 
$$
If the local variance parameters are distributed inverse-gamma
$$
1 / \lambda^2 \sim \text{Gamma}(\nu / 2, \nu / 2) .
$$

Many distributions used in regression shrinkage: Double Exponential (Laplace), and Hierarchical Shrinkage (Horseshoe), have this representation.

You can draw a sample from this:
```{r}
df <- 10
n <- 1000
sigma <- rgamma(n, 0.5 * df, 0.5 * df)
x <- rnorm(n, sd = sqrt(1 / sigma ^ 2))
```

Plot samples drawn in this way against either samples or theoretical values of the Student-t distribution.
Try a few values of the degrees of freedom. 
Try something small (3) and large (100).

You can draw samples directly from a Student-t with `rt`.
A quantile-quantile plot ([geom_qq](http://ggplot2.tidyverse.org/reference/geom_qq.html)) or a density plot with the function ([geom_density](http://ggplot2.tidyverse.org/reference/geom_density.html) and [stat_function](http://ggplot2.tidyverse.org/reference/stat_function.html)).

Note: there isn't a right answer to this.
Well, actually, there is, and you know it. They are equivalent, a proof is in the link.
So for credit, do a little work, and show it.
This pattern appears often, so wrap your head around it.

[^mix]: See [Student-t as a mixture of normals](https://www.johndcook.com/t_normal_mixture.pdf)

# Separation

Continue what was covered in class.


# Transformations of Coefficients

@Rainey2016b notes that unbiased estimators of parameters does not imply that transformations of those parameters are unbiased estimators.

# Poisson and Negative Binomial Example

Fill in
